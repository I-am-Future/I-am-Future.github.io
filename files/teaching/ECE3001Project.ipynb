{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ0yN9xvH_Qi"
      },
      "source": [
        "# ECE3001 Project Part2: Speaker Identification\n",
        "\n",
        "Updated on Mar. 26th, 2024\n",
        "\n",
        "Assignment Maker: Yuejiao Xie, Lai Wei.\n",
        "\n",
        "This notebook is recommended to run on Google Colab.\n",
        "If you want to run it on your local machine, please make sure the required packages are installed. The datasets are downloaded and placed in the correct directory.\n",
        "\n",
        "Follow the PDF description to complete the project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a42o6D28H_Qj"
      },
      "source": [
        "## 1. Prepare the dataset & Install packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2y-8FDaKm25"
      },
      "source": [
        "## 1.1 Mount the Google Drive to this notebook.\n",
        "\n",
        "**Note: You need to run this subsection (1.1) EVERY time you started this notebook.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akJtdDZlH_Qj"
      },
      "outputs": [],
      "source": [
        "# Mount the google drive to this notebook\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Make a new directory in the drive, called \"ECE3001_Project\"\n",
        "!mkdir -p /content/drive/MyDrive/ECE3001_Project\n",
        "\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sULC4JZbJNvU"
      },
      "source": [
        "## 1.2 Download and unzip the datasets\n",
        "\n",
        "**Make sure your Google Drive has around 5GB free space.**\n",
        "\n",
        "Run the following cell to:\n",
        "\n",
        "Download the dataset to the Google Drive. It may take 1-2 minutes.\n",
        "\n",
        "Unzip the dataset. It may take 3-4 minutes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Srcr1ReH_Qk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Download if necessary\n",
        "if os.path.exists('/content/drive/MyDrive/ECE3001_Project/stu_dataset.zip'):\n",
        "  print('The datasets are already at the Google Drive, skip this step...')\n",
        "else:\n",
        "  print('The datasets do NOT exist at the Google Drive, Downloading...')\n",
        "  # Dataset link: https://drive.google.com/file/d/1-0YkEZ3-PXPR5Vfvnpei96kfAb0f3w3Y/view?usp=sharing\n",
        "  !gdown --id 1-0YkEZ3-PXPR5Vfvnpei96kfAb0f3w3Y -O /content/drive/MyDrive/ECE3001_Project/\n",
        "\n",
        "print('Unzipping the datasets...')\n",
        "!unzip -o -q \"/content/drive/MyDrive/ECE3001_Project/stu_dataset.zip\" -d \"/content\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvf5gJnoH_Qk"
      },
      "source": [
        "## 2. Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SupnSiJcoTe2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "import datetime\n",
        "import time\n",
        "# from timm.utils import accuracy\n",
        "\n",
        "import librosa\n",
        "import numpy as np\n",
        "import librosa.display\n",
        "import math\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import IPython.display as ipd\n",
        "\n",
        "import torch.nn as nn\n",
        "from torchvision.models import vgg11, vgg11_bn, vgg13\n",
        "from torchvision.models import resnet18"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyOty6jvH_Ql"
      },
      "source": [
        "## 3. Define datasets & preprocessing helper functions\n",
        "\n",
        "Hint: In most cases, you don't need to modify this part. Just run it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XV9IBC90uZVs"
      },
      "outputs": [],
      "source": [
        "# Loading data\n",
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, data_dir, max_len, window_length, window_shift, use_stft):\n",
        "        self.data_dir = data_dir\n",
        "        self.file_list = os.listdir(data_dir)\n",
        "        self.max_len = max_len\n",
        "        self.window_shift = window_shift\n",
        "        self.window_length = window_length\n",
        "        self.use_stft = use_stft\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filename = os.path.join(self.data_dir, self.file_list[idx])\n",
        "        wav_data = extract_hpss_features_sg(filename, max_length=self.max_len, window_length=self.window_length, window_shift=self.window_shift, use_stft=self.use_stft)\n",
        "        wav_data = torch.tensor(wav_data)\n",
        "        wav_data = wav_data.unsqueeze(0)\n",
        "\n",
        "        # Parse label from filename (filename format: id1_filename.wav)\n",
        "        label = self.file_list[idx].split('_')[0][2:]  # Extract label from filename\n",
        "        label = torch.tensor([int(label)])\n",
        "\n",
        "        return wav_data, label\n",
        "\n",
        "def extract_hpss_features_sg(wav_path, max_length, window_length=320, window_shift=160, use_stft=True):\n",
        "    \"\"\"Extract Harmonic-Percussive Source Separation features.\n",
        "\n",
        "    Args:\n",
        "      wav_dir: string, directory of wavs.\n",
        "      out_dir: string, directory to write out features.\n",
        "      recompute: bool, if True recompute all features, if False skip existed\n",
        "                 extracted features.\n",
        "    \"\"\"\n",
        "    cnt = 0\n",
        "    t1 = time.time()\n",
        "    (audio, sr) = read_audio(wav_path)\n",
        "\n",
        "    if audio.shape[0] == 0:\n",
        "        print(\"File %s is corrupted!\" % wav_path)\n",
        "        raise ValueError\n",
        "    else:\n",
        "        # librosa.display.waveshow(audio, sr=sr)\n",
        "        # plt.show()\n",
        "\n",
        "        if use_stft: # compute stft\n",
        "            spec = np.log(get_spectrogram(audio, window_length, window_shift) + 1e-8)\n",
        "        else: # not use stft\n",
        "            frame = 256\n",
        "            split_num = math.floor(audio.shape[0] / frame)\n",
        "            new_audio = np.split(audio[:split_num*frame], split_num)\n",
        "            spec = np.stack(new_audio, axis=0).T\n",
        "\n",
        "        spec = norm(spec)\n",
        "        spec = spec.T\n",
        "        spec = pad_trunc_seq(spec, max_length)\n",
        "\n",
        "        # cnt += 1\n",
        "    # print(\"Thread %d Extracting feature time: %s\" % (i, (time.time() - t1)))\n",
        "    return spec\n",
        "\n",
        "def read_audio(path, target_fs=None):\n",
        "    try :\n",
        "        audio, fs = librosa.load(path, sr=None) # fs:sample rate\n",
        "    except:\n",
        "        print(path)\n",
        "\n",
        "    if audio.ndim > 1:  # 维度>1，这里考虑双声道的情况，维度为2，在2个维度上取均值，变成单声道\n",
        "        audio = np.mean(audio, axis=1)\n",
        "    if target_fs is not None and fs != target_fs:\n",
        "        audio = librosa.resample(audio, orig_sr=fs, target_sr=target_fs)  # 重采样输入信号，到目标采样频率\n",
        "        fs = target_fs\n",
        "    return audio, fs\n",
        "\n",
        "def pad_trunc_seq(x, max_len):\n",
        "    \"\"\"Pad or truncate a sequence data to a fixed length.\n",
        "\n",
        "    Args:\n",
        "      x: ndarray, input sequence data.\n",
        "      max_len: integer, length of sequence to be padded or truncated.\n",
        "\n",
        "    Returns:\n",
        "      ndarray, Padded or truncated input sequence data.\n",
        "    \"\"\"\n",
        "    L = len(x)\n",
        "    shape = x.shape\n",
        "    if L < max_len:\n",
        "        pad_shape = (max_len - L,) + shape[1:]\n",
        "        pad = np.zeros(pad_shape)\n",
        "        x_new = np.concatenate((x, pad), axis=0)\n",
        "    else:\n",
        "        x_new = x[0:max_len]\n",
        "\n",
        "    return x_new\n",
        "\n",
        "def get_spectrogram(wav, win_length, win_shift):\n",
        "    D = librosa.stft(wav, n_fft=win_length, hop_length=win_shift, win_length=win_length, window='hamming')\n",
        "    spect, phase = librosa.magphase(D)\n",
        "    return spect\n",
        "\n",
        "\n",
        "def norm(spec):\n",
        "    mean = np.reshape(np.mean(spec, axis=1), (spec.shape[0],1))\n",
        "    std = np.reshape(np.std(spec, axis=1), (spec.shape[0],1))\n",
        "    spec = np.divide(np.subtract(spec,np.repeat(mean, spec.shape[1], axis=1)), np.repeat(std, spec.shape[1], axis=1))\n",
        "    return spec\n",
        "\n",
        "def get_one_wave(filename, args):\n",
        "    \"\"\" Get one wave file, return the feature for model input.\n",
        "\n",
        "    Args:\n",
        "        filename: string, the path of the wave file.\n",
        "        args: the args object, containing the parameters for feature extraction.\n",
        "    \"\"\"\n",
        "\n",
        "    wav_data = extract_hpss_features_sg(filename, max_length=args.max_len, window_length=args.window_length, window_shift=args.window_shift, use_stft=args.use_stft)\n",
        "    wav_data = torch.tensor(wav_data)\n",
        "    wav_data = wav_data.unsqueeze(0)\n",
        "    # to feed into the model, the shape should be (batch, channel, time, freq)\n",
        "    wav_data = wav_data.unsqueeze(0)\n",
        "    wav_data = wav_data.to(dtype=torch.float32)\n",
        "    return wav_data\n",
        "\n",
        "# Copied from timm/utils/metrics.py\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    maxk = min(max(topk), output.size()[1])\n",
        "    batch_size = target.size(0)\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n",
        "    return [correct[:min(k, maxk)].reshape(-1).float().sum(0) * 100. / batch_size for k in topk]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdLPLBMZSx-Y"
      },
      "source": [
        "## 4. Define models\n",
        "\n",
        "Hint: In most cases, you don't need to modify this part. Just run it.\n",
        "\n",
        "But you can try to modify the model structure to improve the performance if you want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEPQoWtdvP8-"
      },
      "outputs": [],
      "source": [
        "# Loading Model\n",
        "class vgg_base(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(vgg_base,self).__init__()\n",
        "        self.vggmodel=vgg11(pretrained=False).features\n",
        "        self.vggmodel[0]=nn.Conv2d(input_dim,64,kernel_size = 3, padding= 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.vggmodel(x)\n",
        "        return x\n",
        "\n",
        "class vggbn_base(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(vggbn_base,self).__init__()\n",
        "        self.vggmodel=vgg11_bn(pretrained=False).features\n",
        "        self.vggmodel[0]=nn.Conv2d(input_dim,64,kernel_size = 3, padding= 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.vggmodel(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class resnet_base(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(resnet_base,self).__init__()\n",
        "        self.resnetmodel=resnet18(pretrained=False)\n",
        "        self.resnetmodel.conv1=nn.Conv2d(input_dim,64,kernel_size = 7, stride=2,padding= 3,bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.resnetmodel(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class My_model(nn.Module):\n",
        "    def __init__(self, input_dim=1, num_classes=93, model_base=\"vgg\"):\n",
        "        super(My_model,self).__init__()\n",
        "        if model_base == \"vgg\":\n",
        "            self.backbone=vgg_base(input_dim)\n",
        "            self.linear = nn.Linear(in_features=512, out_features=num_classes)\n",
        "        elif model_base == \"vggbn\":\n",
        "            self.backbone=vggbn_base(input_dim)\n",
        "            self.linear = nn.Linear(in_features=512, out_features=num_classes)\n",
        "        elif model_base ==\"resnet\":\n",
        "            self.backbone=resnet_base(input_dim)\n",
        "            self.linear = nn.Linear(in_features=1000, out_features=num_classes)\n",
        "        else:\n",
        "            raise ValueError(\"model_base should be vgg, vggbn, resnet or transformer\")\n",
        "\n",
        "        self.model_base=model_base\n",
        "        self.avgpool = nn.AvgPool1d(kernel_size=200, stride=1)\n",
        "        self.activate = nn.Softmax(dim=1)\n",
        "        self.criteria = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, input, label=None):\n",
        "        result = self.backbone(input)\n",
        "\n",
        "        if self.model_base in [\"vgg\",\"vggbn\"]:\n",
        "            result = result.view(result.size(0), result.size(1), -1)\n",
        "            result = self.avgpool(result)\n",
        "            result = result.reshape(result.size(0), -1)\n",
        "\n",
        "        result = self.linear(result)\n",
        "        result = self.activate(result)\n",
        "\n",
        "        _, pred_label = result.max(-1)\n",
        "\n",
        "        if label is not None: # train\n",
        "            loss = self.criteria(result, label.view(-1))\n",
        "            return loss, result, pred_label\n",
        "        else: # test\n",
        "            return result, pred_label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PaHSsP6H_Qm"
      },
      "source": [
        "## 5. Define procedures for training and testing\n",
        "\n",
        "Hint: In most cases, you don't need to modify this part. Just run it.\n",
        "\n",
        "But if you want to trace the training process (e.g., save validation accuracy at each epoch), you can modify the `train` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBw5KbVhoYLV"
      },
      "outputs": [],
      "source": [
        "def valid(args, model):\n",
        "    audio_testset = AudioDataset(args.test_path, args.max_len, args.window_length, args.window_shift, args.use_stft)\n",
        "    test_data = DataLoader(audio_testset, batch_size=args.batchsize, shuffle=False, num_workers=2)\n",
        "\n",
        "    model.eval()\n",
        "    acc1_total = 0.\n",
        "    acc5_total = 0.\n",
        "    step = 0\n",
        "\n",
        "    print(\"[Valid] : Start validation...\")\n",
        "    with torch.no_grad():\n",
        "        for step, (x, label) in enumerate(tqdm(test_data)):\n",
        "            x = x.to(dtype=torch.float32, device=args.device)\n",
        "            label = label.to(args.device)\n",
        "            result, pred = model(x)\n",
        "            acc1, acc5 = accuracy(result, label.view(-1), topk=(1, 5))\n",
        "            acc1, acc5 = acc1.item()/100, acc5.item()/100\n",
        "            # loss_total += float(loss.item())\n",
        "            acc1_total += acc1\n",
        "            acc5_total += acc5\n",
        "\n",
        "    print(\"[Valid] : Valid_acc1:{}, Valid_acc5: {}\".format( acc1_total / (step+1), acc5_total / (step+1) ))\n",
        "    return acc1_total / (step+1), acc5_total / (step+1)\n",
        "\n",
        "\n",
        "def train(args, model):\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-5)\n",
        "    best_acc1 = 0\n",
        "    best_acc5 = 0\n",
        "\n",
        "    audio_trainset = AudioDataset(args.train_path, args.max_len, args.window_length, args.window_shift, args.use_stft)\n",
        "    print(f\"[Train] : Length of training set: {len(audio_trainset)}\")\n",
        "    train_data = DataLoader(audio_trainset, batch_size=args.batchsize, shuffle=True, drop_last=True, num_workers=2)\n",
        "\n",
        "    print(\"[Train] : Start training...\")\n",
        "    for epoch in range(args.epochs):\n",
        "        print(f\"[Train] : Training epoch {epoch}...\")\n",
        "        model.train()\n",
        "        acc1_total = 0.\n",
        "        acc5_total = 0.\n",
        "        loss_total = 0.\n",
        "\n",
        "        tbar = tqdm(train_data)\n",
        "        for step, (x, label) in enumerate(tbar):\n",
        "            x = x.to(dtype=torch.float32, device=args.device)\n",
        "            label = label.to(args.device)\n",
        "            optimizer.zero_grad()\n",
        "            loss, result, pred = model(x, label)\n",
        "            acc1, acc5 = accuracy(result, label.view(-1), topk=(1, 5))\n",
        "\n",
        "            acc1, acc5 = acc1.item() / 100, acc5.item() / 100\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            acc1_total += acc1\n",
        "            acc5_total += acc5\n",
        "            loss_total += float(loss.item())\n",
        "\n",
        "            if step % args.print_every == 0:\n",
        "                tbar.set_postfix_str('epoch %d, step %d, step_loss %.4f, step_acc1 %.4f, step_acc5 %.4f' %\n",
        "                                    (epoch, step, loss_total/(step+1), acc1_total/(step+1), acc5_total/(step+1)))\n",
        "\n",
        "        acc1, acc5 = valid(args, model)\n",
        "        # Hint: you may want to save the acc1 and acc5 for each epoch and plot them later\n",
        "        # so you need to create a list before training loop, and append the acc1 and acc5 to the list\n",
        "        # finally, you can return the list for plotting\n",
        "\n",
        "        if acc1 > best_acc1:\n",
        "            best_acc1 = acc1\n",
        "            best_acc5 = acc5\n",
        "            # pt_filename = args.model_base + \"_best.pt\"\n",
        "            pt_filename = f\"{args.model_base}_{'w' if args.use_stft else 'wo'}_stft_best.pt\"\n",
        "            pt_filename = args.save_path + '/' + pt_filename\n",
        "            print('Achieve best acc1: %.4f, acc5: %.4f, epoch: %d. Saving to `%s`...' % (acc1, acc5, epoch, pt_filename))\n",
        "            torch.save(model.state_dict(), pt_filename)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBKupd2sviEr"
      },
      "source": [
        "## 6. Start model training!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KftlT4R5H_Qm"
      },
      "source": [
        "For this project, you need to at least run two experiments: one with `use_stft=True` and the other one with `use_stft=False`.\n",
        "\n",
        "We recommend you to change the `model_base` to other models (e.g., `vggbn`), to see if the performance can be improved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNoyPUo8wAdb"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "if device == 'cpu':\n",
        "    raise Warning(\"[From ECE3001 TA:] Your device is CPU, training will be very very slow. Please change the runtime to GPU.\")\n",
        "\n",
        "args = argparse.Namespace(\n",
        "    device=device,                  # device, do not change\n",
        "\n",
        "    ## path parameters\n",
        "    train_path='./stu_dataset/train', # training dataset path, do not change usually\n",
        "    test_path='./stu_dataset/test', # testing dataset path, do not change usually\n",
        "    save_path='/content/drive/MyDrive/ECE3001_Project/',\n",
        "\n",
        "    ## training parameters, you may change some of them\n",
        "    epochs=20,                      # number of epochs to train\n",
        "    print_every=10,                 # print training information every print_every steps\n",
        "    batchsize=16,                   # batch size\n",
        "    lr=1e-4,                        # learning rate\n",
        "    model_base=\"resnet\",            # model base: vgg, resnet, vggbn\n",
        "\n",
        "    ## data processing parameters, you may change some of them\n",
        "    max_len=800,                    # max length of audio\n",
        "    window_shift=256,               # hop shift\n",
        "    window_length=510,              # window length\n",
        "    use_stft=True,                  # whether to use stft\n",
        ")\n",
        "\n",
        "\n",
        "model = My_model(num_classes=92, model_base=args.model_base)\n",
        "model = model.to(dtype=torch.float32, device=args.device)\n",
        "\n",
        "train(args, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3AhkVPyH_Qn"
      },
      "source": [
        "## 7. Model evaluation\n",
        "\n",
        "At here, you can dirrectly get the prediction results from the trained model.\n",
        "\n",
        "Suppose you have trained the model and saved it (e.g., `resnet_w_stft_best.pt`).\n",
        "\n",
        "You can set the `model_base` and the trained `model_path` below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RU8No_fIH_Qn"
      },
      "outputs": [],
      "source": [
        "test_args = argparse.Namespace(\n",
        "    device=device,                  # device, do not change\n",
        "    batchsize=16,\n",
        "\n",
        "    ## path parameters\n",
        "    train_path='./stu_dataset/train', # training dataset path, do not change usually\n",
        "    test_path='./stu_dataset/test', # testing dataset path, do not change usually\n",
        "    save_path='/content/drive/MyDrive/ECE3001_Project/',\n",
        "\n",
        "    ## data processing parameters, you may change some of them\n",
        "    max_len=800,                    # max length of audio\n",
        "    window_shift=256,               # hop shift\n",
        "    window_length=510,              # window length\n",
        "    use_stft=True,                  # whether to use stft\n",
        "\n",
        "\n",
        "    model_base=\"resnet\",            # model base: vgg, resnet, vggbn\n",
        "    ## The model to be evaluated's path.\n",
        "    model_path='resnet_w_stft_best.pt'\n",
        ")\n",
        "\n",
        "model = My_model(num_classes=92, model_base=test_args.model_base)\n",
        "model.load_state_dict(torch.load(test_args.save_path + test_args.model_path))\n",
        "model.eval()\n",
        "\n",
        "print('Model (type \"%s\") loaded from `%s` successfully!' % (test_args.model_base, test_args.model_path))\n",
        "\n",
        "# Simply run validation on the testing dataset\n",
        "model = model.to(device)\n",
        "valid(test_args, model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}